{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Die NLP-Pipeline\n",
    "\n",
    "Vom Text zur Wissen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"nlp_pipeline.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Korpora\n",
    "<span style=\"color:blue\">*raw text*</span>\n",
    "- Sammlung von (natürlichsprachigen) Texten\n",
    "- Wichtigster 'Rohstoff' in der Computerlinguistik\n",
    "- Oft für bestimmte Aufgaben zusammengestellt\n",
    "  - Machine Translation, Sentiment Analysis, Genre Identification, ...\n",
    "- Oft aus bestimmten Quellen zusammengestellt\n",
    "  - Newswire, Movie Reviews, Tweets, Emails, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Beispiele:\n",
    "- Brown (Nur Text)\n",
    "- CoNLL2002 (Named Entity Recognition)\n",
    "- Reuters (Dokumentenklassifikation)\n",
    "- IMDB Reviews (Sentiment-Analyse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Einlesen von Korpora mit NLTK\n",
    "\n",
    "```python\n",
    "from nltk.corpus import gutenberg\n",
    "```\n",
    "\n",
    "nltk.corpus enthält viele bekannte Korpora und stellt viele Dinge sofort zur Verfügung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma: [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of exi [...]\n",
      "\n",
      "Characters: 887071\n"
     ]
    }
   ],
   "source": [
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "print(\"Emma:\",emma[:190],\"[...]\\n\")\n",
    "print(\"Characters:\",len(emma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stopwort - Korpora\n",
    "\n",
    "Welches Wort enthält mehr Bedeutung?\n",
    "\n",
    "-> Welches Wort enthält mehr Information über den Inhalt und Kontext\n",
    "\n",
    "> 1) Der \\\n",
    "> 2) Kapitän\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**2) Kapitän** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his'] \n",
      "\n",
      "Is 'and' in 'stopwords'? True\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:20],\"\\n\")\n",
    "print(\"Is 'and' in 'stopwords'?\",\"and\" in stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentierung und Tokenisierung *(Chunking)*\n",
    "\n",
    "<span style=\"color:blue\">*tokenized sentences*</span>\n",
    "\n",
    "Unser Text ist bislang ein einziger, langer String.\n",
    "\n",
    "```xml\n",
    "\"Emma Woodhouse, handsome, clever, and rich, [...]\"\n",
    "``` \n",
    "\n",
    "Wir wollen wissen, wo Untereinheiten ('Chunks') beginnen und enden:\n",
    "- Sätze\n",
    "- Wörter\n",
    "- Phrasen\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Wir wollen wissen, wo Untereinheiten ('Chunks') beginnen und enden:\n",
    "- Sätze\n",
    "- Wörter\n",
    "- Phrasen\n",
    "- ...\n",
    "\n",
    "> Ideen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Methoden\n",
    "- Regelbasiert\n",
    "    - *Wenn* <span style=\"background-color: #ffffff\">.</span>*, dann Satzende.*\n",
    "    - Benötigt Experten\n",
    "- Statistisch\n",
    "    - <span style=\"background-color: #ffffff\">. A</span> *ist wahrscheinlich Satzende.*\n",
    "    - Benötigt Trainingsdaten\n",
    "- Neuronal\n",
    "    - Benötigt Trainingsdaten und viel Rechenleistung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenisierung in NLTK\n",
    "\n",
    "NLTK stellt Standard-Implementierungen zur Verfügung:\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.']\n",
      "['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and']\n",
      "Number of Sentences: 7493\n",
      "Number of Words: 191785\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "emma_sents = sent_tokenize(emma) # TODO\n",
    "emma_words = word_tokenize(emma) # TODO\n",
    "print(emma_sents[0:1])\n",
    "print(emma_words[58:75])\n",
    "print(\"Number of Sentences:\",len(emma_sents)) #TODO\n",
    "print(\"Number of Words:\",len(emma_words)) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Aufgabe: Tokenisiere den Emma-Text als Liste von Sätzen, wobei jeder Satz eine Liste von Wörtern ist.\n",
    "\n",
    "```xml\n",
    "[[..][..][..][..],...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = emma\n",
    "\n",
    "tokenized_sents = None # TODO\n",
    "\n",
    "print(tokenized_sents[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stopwort - Entfernung mit NLTK\n",
    "\n",
    "Für viele Aufgaben sind die Stoppwörter unwichtig, oder sogar hinderlich (Rauschen, Datenmenge, ...) \\\n",
    "Dann wird als Vorverarbeitungsschritt und als Teil der NLP-Pipeline eine Stopwort-Entfernung durchgeführt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword fraction guess: 32 %\n",
      "Actual Emma Stopword fraction: 36 %\n",
      "Wow, very accurate!\n"
     ]
    }
   ],
   "source": [
    "emma_filtered = [w for w in emma if w.lower() not in stopwords] # Together\n",
    "remaining_fraction = len(emma_filtered) / len(emma)\n",
    "stopword_fraction = round(100 -(fraction*100))\n",
    "guess =  32 # TODO 0% - 100%\n",
    "print(\"Stopword fraction guess:\",guess,\"%\")\n",
    "print(\"Actual Emma Stopword fraction:\",stopword_fraction,\"%\")\n",
    "print(\"Wow, very accurate!\" if (abs(stopword_fraction - guess) < 5) else \"Almost. Surprising, isn't it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "Typischerweise der zweite Schritt der NLP-Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Was ist ein Part-of-Speech Tag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "refuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "they refuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**VB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the refuse permit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**NN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Part-of-Speech Tagging mit NLTK\n",
    "\n",
    "```python\n",
    "from nltk import pos_tag\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "pos_tagged_text = nltk.pos_tag(text) # TODO\n",
    "print(pos_tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pos_tagged_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Aufgabe: Tagge den tokenisierten Emma-Text mit seinen Part-of-Speech Labels.\n",
    "\n",
    "```xml\n",
    "[[(word,pos),(word,pos),...][..][..][..],...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "NOTE: Als Aufgabe am Ende: Finde andere Tagger und vergleiche die Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "text = tokenized_sents\n",
    "\n",
    "pos_tagged_sents = None # TODO\n",
    "\n",
    "print(tokenized_sents[1:2])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
