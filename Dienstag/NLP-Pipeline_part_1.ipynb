{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Die NLP-Pipeline\n",
    "\n",
    "Vom Text zur Wissen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"nlp_pipeline.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Korpora\n",
    "<span style=\"color:blue\">*raw text*</span>\n",
    "- Sammlung von (natürlichsprachigen) Texten\n",
    "- Wichtigster 'Rohstoff' in der Computerlinguistik\n",
    "- Oft für bestimmte Aufgaben zusammengestellt\n",
    "  - Machine Translation, Sentiment Analysis, Genre Identification, ...\n",
    "- Oft aus bestimmten Quellen zusammengestellt\n",
    "  - Newswire, Movie Reviews, Tweets, Emails, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Beispiele:\n",
    "- Brown (Nur Text)\n",
    "- CoNLL2002 (Named Entity Recognition)\n",
    "- Reuters (Dokumentenklassifikation)\n",
    "- IMDB Reviews (Sentiment-Analyse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Einlesen von Korpora mit NLTK\n",
    "\n",
    "```python\n",
    "from nltk.corpus import gutenberg\n",
    "```\n",
    "\n",
    "nltk.corpus enthält viele bekannte Korpora und stellt viele Dinge sofort zur Verfügung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "print(\"Emma:\",emma[:190],\"[...]\\n\")\n",
    "print(\"Characters:\",len(emma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stopwort - Korpora\n",
    "\n",
    "Welches Wort enthält mehr Bedeutung?\n",
    "\n",
    "-> Welches Wort enthält mehr Information über den Inhalt und Kontext\n",
    "\n",
    "> 1) Der \\\n",
    "> 2) Kapitän\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**2) Kapitän** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:20],\"\\n\")\n",
    "print(\"Is 'and' in 'stopwords'?\",\"and\" in stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentierung und Tokenisierung *(Chunking)*\n",
    "\n",
    "<span style=\"color:blue\">*tokenized sentences*</span>\n",
    "\n",
    "Unser Text ist bislang ein einziger, langer String.\n",
    "\n",
    "```xml\n",
    "\"Emma Woodhouse, handsome, clever, and rich, [...]\"\n",
    "``` \n",
    "\n",
    "Wir wollen wissen, wo Untereinheiten ('Chunks') beginnen und enden:\n",
    "- Sätze\n",
    "- Wörter\n",
    "- Phrasen\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Wir wollen wissen, wo Untereinheiten ('Chunks') beginnen und enden:\n",
    "- Sätze\n",
    "- Wörter\n",
    "- Phrasen\n",
    "- ...\n",
    "\n",
    "> Ideen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Methoden\n",
    "- Regelbasiert\n",
    "    - *Wenn* <span style=\"background-color: #ffffff\">.</span>*, dann Satzende.*\n",
    "    - Benötigt Experten\n",
    "- Statistisch\n",
    "    - <span style=\"background-color: #ffffff\">. A</span> *ist wahrscheinlich Satzende.*\n",
    "    - Benötigt Trainingsdaten\n",
    "- Neuronal\n",
    "    - Benötigt Trainingsdaten und viel Rechenleistung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenisierung in NLTK\n",
    "\n",
    "NLTK stellt Standard-Implementierungen zur Verfügung:\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "emma_sents =  # TODO\n",
    "emma_words =  # TODO\n",
    "print(emma_sents[0:1])\n",
    "print(emma_words[58:75])\n",
    "print(\"Number of Sentences:\",) #TODO\n",
    "print(\"Number of Words:\",) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Aufgabe: Tokenisiere den Emma-Text als Liste von Sätzen, wobei jeder Satz eine Liste von Wörtern ist.\n",
    "\n",
    "```xml\n",
    "[[..][..][..][..],...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = emma\n",
    "\n",
    "tokenized_sents =  # TODO\n",
    "\n",
    "print(tokenized_sents[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stopwort - Entfernung mit NLTK\n",
    "\n",
    "Für viele Aufgaben sind die Stoppwörter unwichtig, oder sogar hinderlich (Rauschen, Datenmenge, ...) \\\n",
    "Dann wird als Vorverarbeitungsschritt und als Teil der NLP-Pipeline eine Stopwort-Entfernung durchgeführt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "emma_filtered =  # Together\n",
    "remaining_fraction = len(emma_filtered) / len(emma)\n",
    "stopword_fraction = round(100 -(fraction*100))\n",
    "guess = None # TODO 0% - 100%\n",
    "print(\"Stopword fraction guess:\",guess,\"%\")\n",
    "print(\"Actual Emma Stopword fraction:\",stopword_fraction,\"%\")\n",
    "print(\"Wow, very accurate!\" if (abs(stopword_fraction - guess) < 5) else \"Almost. Surprising, isn't it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"nlp_pipeline.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "Typischerweise der zweite Schritt der NLP-Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Was ist ein Part-of-Speech Tag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "refuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "they refuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**VB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the refuse permit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**NN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Part-of-Speech Tagging mit NLTK\n",
    "\n",
    "```python\n",
    "from nltk import pos_tag\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "pos_tagged_text =  # TODO\n",
    "print(pos_tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "type(pos_tagged_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Aufgabe: Tagge den tokenisierten Emma-Text mit seinen Part-of-Speech Labels.\n",
    "\n",
    "```xml\n",
    "[[(word,pos),(word,pos),...][..][..][..],...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "NOTE: Als optionale Aufgabe am Ende: Finde andere Tagger und vergleiche die Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "text = tokenized_sents\n",
    "\n",
    "pos_tagged_sents =  # TODO\n",
    "\n",
    "print(tokenized_sents[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"nlp_pipeline.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reportedly, last Saturday, Santa was sighted flying directly above the headquarters of Denver-Sled Inc. in Denver, Colorado.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reportedly, last <span style=\"color:blue\">**Saturday**</span>, <span style=\"color:red\">**Santa**</span> was sighted flying directly above the headquarters of <span style=\"color:green\">**Denver-Sled Inc.**</span> in <span style=\"color:purple\">**Denver**</span>, <span style=\"color:purple\">**Colorado**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reportedly, <span style=\"color:blue\">**last Saturday**</span>, <span style=\"color:red\">**Santa**</span> was sighted flying directly above the headquarters of <span style=\"color:green\">**Denver-Sled Inc.**</span> in <span style=\"color:purple\">**Denver, Colorado**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Discuss: Wich of the above is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Named Entity Types\n",
    "\n",
    "- Können sehr allgemein oder sehr genau sein - **Location** vs. **City-District**\n",
    "- Immer dem Task angepasst\n",
    "- Generell: Je spezifischer, desto mehr Fehler\n",
    "\n",
    "Häufig z.B.:\n",
    "![ne_types](NE_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**NER** kann in zwei Unteraufgaben unterteilt werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " 1. NE-Grenzen erkennen: \"*wo*\"\n",
    " 2. NEs klassifizieren: \"*was*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "--> Ein Klassifikator setzt nur B-I-O Marker in den Text\n",
    "\n",
    "Beginning / Inside / Outside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "     O        B      I       B    O     O   \n",
    "Reportedly, last Saturday, Santa was sighted```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "--> Ein zweiter Klassifikator klassifiziert jetzt die Markierten Named Entities\n",
    "\n",
    "```\n",
    "     /          DATE        PER   /     /   \n",
    "Reportedly, last Saturday, Santa was sighted```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Moderne, vor allem neuronale, Ansätze versuchen auch schon, beides auf einmal zu machen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NER mit NLTK\n",
    "\n",
    "```python\n",
    "from nltk import ne_chunk\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sent = \"Reportedly, last Saturday, Santa was sighted flying directly above the headquarters of Denver-Sled Inc. in Denver, Colorado.\"\n",
    "split_sent = sent.split()\n",
    "tagged_sent = pos_tag(sent.split())\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "ne_tagged_sent = nltk.ne_chunk(tagged_sent)\n",
    "print(ne_tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ne_tagged_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"nlp_pipeline.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
